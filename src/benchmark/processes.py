import abc
import os
import platform


class process(metaclass=abc.ABCMeta):
    def __init__(self, test, executor, log):
        self.__my_log = log
        self._my_test = test
        self._my_executor = executor
        self._my_output = None
        self._my_row_output = None

    def __print_error(self):
        out = self._my_output
        iserror = False
        for line in out:
            if line.rfind('ERROR! :') != -1:
                iserror = True
                self.__my_log.error('    {0}'.format(line[8:]))
                continue
            if iserror:
                self.__my_log.error('    {0}'.format(line))

    @abc.abstractmethod
    def _fill_command_line(self):
        pass

    @staticmethod
    def _get_cmd_python_version():
        cmd_python_version = ''
        os_type = platform.system()
        if os_type == 'Linux':
            cmd_python_version = 'python3'
        else:
            cmd_python_version = 'python'
        return cmd_python_version

    def get_model_shape(self):
        input_shape = []
        for line in self._my_output:
            if 'Shape for input layer' in line:
                input_shape.append(line.split(':')[-1].strip())
        return ', '.join(input_shape) if len(input_shape) > 0 else 'Undefined'

    def execute(self):
        command_line = self._fill_command_line()
        if command_line == '':
            self.__my_log.error('Command line is empty')
        self.__my_log.info('Start inference test on model : {}'.format(self._my_test.model.name))
        self._my_executor.set_target_framework(self._my_test.indep_parameters.inference_framework)
        self._my_row_output = self._my_executor.execute_process(command_line)
        self._my_output = self._my_row_output[1]

        if type(self._my_output) is not list:
            self._my_output = self._my_output.decode("utf-8").split('\n')[:-1]

        if self._my_row_output[0] == 0:
            self.__my_log.info('End inference test on model : {}'.format(self._my_test.model.name))
        else:
            self.__my_log.warning('Inference test on model: {} was ended with error. Process logs:'.format(self._my_test.model.name))
            self.__print_error()

    def get_status(self):
        return self._my_row_output[0]

    @staticmethod
    def get_process(test, executor, log):
        if test.indep_parameters.inference_framework == 'OpenVINO DLDT':
            return OpenVINO_process.create_process(test, executor, log)
        elif test.indep_parameters.inference_framework == 'Caffe':
            return IntelCaffe_process.create_process(test, executor, log)
        elif test.indep_parameters.inference_framework == 'TensorFlow':
            return TensorFlow_process.create_process(test, executor, log)
        else:
            raise ValueError('Invalid framework name: only \'OpenVINO DLDT\', \'Caffe\' and \'TensorFlow\' are available')

    @abc.abstractmethod
    def get_performance_metrics(self):
        pass


class OpenVINO_process(process):
    def __init__(self, test, executor, log):
        super().__init__(test, executor, log)

    @staticmethod
    def __add_extension_for_cmd_line(command_line, extension):
        return '{0} -l {1}'.format(command_line, extension)

    @staticmethod
    def __add_nthreads_for_cmd_line(command_line, nthreads):
        return '{0} -nthreads {1}'.format(command_line, nthreads)

    @staticmethod
    def __add_raw_output_time_for_cmd_line(command_line, raw_output):
        return '{0} {1}'.format(command_line, raw_output)

    def _fill_command_line(self):
        model_xml = self._my_test.model.model
        model_bin = self._my_test.model.weight
        dataset = self._my_test.dataset.path
        batch = self._my_test.indep_parameters.batch_size
        device = self._my_test.indep_parameters.device
        iteration = self._my_test.indep_parameters.iteration

        command_line = '-m {0} -w {1} -i {2} -b {3} -d {4} -ni {5}'.format(model_xml, model_bin, dataset, batch, device, iteration)

        extension = self._my_test.dep_parameters.extension
        if extension:
            command_line = OpenVINO_process.__add_extension_for_cmd_line(command_line, extension)
        nthreads = self._my_test.dep_parameters.nthreads
        if nthreads:
            command_line = OpenVINO_process.__add_nthreads_for_cmd_line(command_line, nthreads)
        command_line = OpenVINO_process.__add_raw_output_time_for_cmd_line(command_line, '--raw_output true')
        return command_line

    @staticmethod
    def create_process(test, executor, log):
        mode = (test.dep_parameters.mode).lower()
        if mode == 'sync':
            return sync_OpenVINO_process(test, executor, log)
        elif mode == 'async':
            return async_OpenVINO_process(test, executor, log)


class sync_OpenVINO_process(OpenVINO_process):
    def __init__(self, test, executor, log):
        super().__init__(test, executor, log)

    def _fill_command_line(self):
        path_to_sync_scrypt = os.path.normpath(os.path.join(
            self._my_executor.get_path_to_inference_folder(),
            'inference_sync_mode.py')
        )
        python = process._get_cmd_python_version()
        common_params = super()._fill_command_line()
        command_line = '{0} {1} {2}'.format(python, path_to_sync_scrypt, common_params)
        return command_line

    def get_performance_metrics(self):
        if self._my_row_output[0] != 0 or len(self._my_output) == 0:
            return None, None, None

        result = self._my_output[-1].strip().split(',')
        average_time = float(result[0])
        fps = float(result[1])
        latency = float(result[2])
        return average_time, fps, latency


class async_OpenVINO_process(OpenVINO_process):
    def __init__(self, test, executor, log):
        super().__init__(test, executor, log)

    @staticmethod
    def __add_nstreams_for_cmd_line(command_line, nstreams):
        return '{0} -nstreams {1}'.format(command_line, nstreams)

    @staticmethod
    def __add_requests_for_cmd_line(command_line, requests):
        return '{0} --requests {1}'.format(command_line, requests)

    def _fill_command_line(self):
        path_to_async_scrypt = os.path.normpath(os.path.join(
            self._my_executor.get_path_to_inference_folder(),
            'inference_async_mode.py')
        )
        python = process._get_cmd_python_version()
        common_params = super()._fill_command_line()
        command_line = '{0} {1} {2}'.format(python, path_to_async_scrypt, common_params)
        nstreams = self._my_test.dep_parameters.nstreams
        if nstreams:
            command_line = async_OpenVINO_process.__add_nstreams_for_cmd_line(command_line, nstreams)
        requests = self._my_test.dep_parameters.async_request
        if requests:
            command_line = async_OpenVINO_process.__add_requests_for_cmd_line(command_line, requests)
        return command_line

    def get_performance_metrics(self):
        if self._my_row_output[0] != 0 or len(self._my_output) == 0:
            return None, None, None

        result = self._my_output[-1].strip().split(',')
        average_time = float(result[0])
        fps = float(result[1])
        return average_time, fps, 0


class IntelCaffe_process(process):
    def __init__(self, test, executor, log):
        super().__init__(test, executor, log)

    @staticmethod
    def __add_channel_swap_for_cmd_line(command_line, channel_swap):
        return '{0} --channel_swap {1}'.format(command_line, channel_swap)

    @staticmethod
    def __add_mean_for_cmd_line(command_line, mean):
        return '{0} --mean {1}'.format(command_line, mean)

    @staticmethod
    def __add_input_scale_for_cmd_line(command_line, input_scale):
        return '{0} --input_scale {1}'.format(command_line, input_scale)

    @staticmethod
    def __add_raw_output_time_for_cmd_line(command_line, raw_output):
        return '{0} {1}'.format(command_line, raw_output)

    def _fill_command_line(self):
        path_to_intelcaffe_scrypt = os.path.normpath(os.path.join(
            self._my_executor.get_path_to_inference_folder(),
            'inference_caffe.py')
        )
        python = process._get_cmd_python_version()

        model_prototxt = self._my_test.model.model
        model_caffemodel = self._my_test.model.weight
        dataset = self._my_test.dataset.path
        batch = self._my_test.indep_parameters.batch_size
        device = self._my_test.indep_parameters.device
        iteration = self._my_test.indep_parameters.iteration

        common_params = '-m {0} -w {1} -i {2} -b {3} -d {4} -ni {5}'.format(model_prototxt, model_caffemodel, dataset, batch, device, iteration)
        channel_swap = self._my_test.dep_parameters.channel_swap
        if channel_swap:
            common_params = IntelCaffe_process.__add_channel_swap_for_cmd_line(common_params, channel_swap)
        mean = self._my_test.dep_parameters.mean
        if mean:
            common_params = IntelCaffe_process.__add_mean_for_cmd_line(common_params, mean)
        input_scale = self._my_test.dep_parameters.input_scale
        if input_scale:
            common_params = IntelCaffe_process.__add_input_scale_for_cmd_line(common_params, input_scale)
        common_params = IntelCaffe_process.__add_raw_output_time_for_cmd_line(common_params, '--raw_output true')
        command_line = '{0} {1} {2}'.format(python, path_to_intelcaffe_scrypt, common_params)
        return command_line

    def get_performance_metrics(self):
        if self._my_row_output[0] != 0 or len(self._my_output) == 0:
            return None, None, None

        result = self._my_output[-1].strip().split(',')
        average_time = float(result[0])
        fps = float(result[1])
        latency = float(result[2])
        return average_time, fps, latency

    @staticmethod
    def create_process(test, executor, log):
        return IntelCaffe_process(test, executor, log)


class TensorFlow_process(process):
    def __init__(self, test, executor, log):
        super().__init__(test, executor, log)

    @staticmethod
    def __add_channel_swap_for_cmd_line(command_line, channel_swap):
        return '{0} --channel_swap {1}'.format(command_line, channel_swap)

    @staticmethod
    def __add_mean_for_cmd_line(command_line, mean):
        return '{0} --mean {1}'.format(command_line, mean)

    @staticmethod
    def __add_input_scale_for_cmd_line(command_line, input_scale):
        return '{0} --input_scale {1}'.format(command_line, input_scale)

    @staticmethod
    def __add_input_shape_for_cmd_line(command_line, input_shape):
        return '{0} --input_shape {1}'.format(command_line, input_shape)

    @staticmethod
    def __add_input_name_for_cmd_line(command_line, input_name):
        return '{0} --input_name {1}'.format(command_line, input_name)

    @staticmethod
    def __add_output_names_for_cmd_line(command_line, output_names):
        return '{0} --output_names {1}'.format(command_line, output_names)

    @staticmethod
    def __add_raw_output_time_for_cmd_line(command_line, raw_output):
        return '{0} {1}'.format(command_line, raw_output)

    def _fill_command_line(self):
        path_to_tensorflow_scrypt = os.path.normpath(os.path.join(
            self._my_executor.get_path_to_inference_folder(),
            'inference_tensorflow.py')
        )
        python = process._get_cmd_python_version()

        model = self._my_test.model.model
        dataset = self._my_test.dataset.path
        batch = self._my_test.indep_parameters.batch_size
        device = self._my_test.indep_parameters.device
        iteration = self._my_test.indep_parameters.iteration

        common_params = '-m {0} -i {1} -b {2} -d {3} -ni {4}'.format(model, dataset, batch, device, iteration)
        channel_swap = self._my_test.dep_parameters.channel_swap
        if channel_swap:
            common_params = TensorFlow_process.__add_channel_swap_for_cmd_line(common_params, channel_swap)
        mean = self._my_test.dep_parameters.mean
        if mean:
            common_params = TensorFlow_process.__add_mean_for_cmd_line(common_params, mean)
        input_scale = self._my_test.dep_parameters.input_scale
        if input_scale:
            common_params = TensorFlow_process.__add_input_scale_for_cmd_line(common_params, input_scale)
        input_shape = self._my_test.dep_parameters.input_shape
        if input_shape:
            common_params = TensorFlow_process.__add_input_shape_for_cmd_line(common_params, input_shape)
        input_name = self._my_test.dep_parameters.input_name
        if input_name:
            common_params = TensorFlow_process.__add_input_name_for_cmd_line(common_params, input_name)
        output_names = self._my_test.dep_parameters.output_names
        if output_names:
            common_params = TensorFlow_process.__add_output_names_for_cmd_line(common_params, output_names)
        common_params = TensorFlow_process.__add_raw_output_time_for_cmd_line(common_params, '--raw_output true')
        command_line = '{0} {1} {2}'.format(python, path_to_tensorflow_scrypt, common_params)
        return command_line

    def get_performance_metrics(self):
        if self._my_row_output[0] != 0 or len(self._my_output) == 0:
            return None, None, None

        result = self._my_output[-1].strip().split(',')
        average_time = float(result[0])
        fps = float(result[1])
        latency = float(result[2])
        return average_time, fps, latency

    @staticmethod
    def create_process(test, executor, log):
        return TensorFlow_process(test, executor, log)
